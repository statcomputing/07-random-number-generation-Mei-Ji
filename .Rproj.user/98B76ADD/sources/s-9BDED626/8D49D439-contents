---
title: "Rejection sampling"
output: pdf_document
author: "Yufan Cao"

exercise 5.3.1
    In this assignment we will express how to use the rejection sampling to sample from some distributions using what we've learned.
---

# 1 Rejection sampling

Let $f$ and $g$ be two probability densities on $(0,\infty)$, such that
\begin{align*}
  f(x) \propto \sqrt{4+x}\,x^{\theta-1} e^{-x}, \quad
  g(x) \propto (2 x^{\theta-1} + x^{\theta-1/2}) e^{-x}, \quad x>0.
\end{align*}

- Find the value of the normalizing constant for $g$, i.e.,
  the constant $C$ such that
\begin{align*}
  C\int_0^\infty (2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x}  dx=1.
\end{align*}
  Show that $g$ is a mixture of Gamma distributions.  Identify the
  component distributions and their weights in the mixture.
- Design a procedure (pseudo-code) to sample from $g$;
  implement it in an R function;
  draw a sample of size $n = 10,000$ using your function for at least one
  $\theta$ value; 
  plot the kernel density estimation of $g$ from your sample and the true
  density in one figure. 
- Design a procedure (pseudo-code) to use rejection sampling to sample from
  $f$ using $g$ as the instrumental distribution.  Overlay the estimated
  kernel density of a random sample generated by your procedure and $f$.
  
# 2 Find the value of the normalizing constant

In this question, we need to calculate the normalizing constant for $g$
\begin{align*}
&\int_0^\infty (2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x}dx \\
&=\int_0^\infty 2 x^{\theta-1}e^{-x}dx + \int_0^\infty x^{\theta-1/2} e^{-x}dx\\
&=2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})
\end{align*}
so that we can get
\[C= \frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})} \]
next we get the full expression of $g$:
\[g(x) = \frac{1}{2\Gamma(\theta)+\Gamma(\theta+\frac{1}{2})}(2 x^{\theta-1} +  x^{\theta-1/2}) e^{-x}\]
we can change it to another expression:
\[g(x) =  \frac{2 \Gamma(\theta)}{2 \Gamma(\theta) + \Gamma(\theta + \frac{1}{2})} \frac{x^{\theta - 1} e^{-x}}{\Gamma(\theta)}  +  \frac{\Gamma(\theta + \frac{1}{2})}{2 \Gamma(\theta) + \Gamma(\theta + \frac{1}{2})}\frac{x^{(\theta + \frac{1}{2}) - 1} e^{-x}}{\Gamma(\theta + \frac{1}{2})} \]
\newpage
and that $g$ is a mixture of Gamma distributions,the component distribution is $\Gamma(\theta, 1)$ with weight $w1$, and $\Gamma(\theta + \frac{1}{2}, 1)$ with weight $w2$:
\[w1 = \frac{2 \Gamma(\theta)}{2 \Gamma(\theta) + \Gamma(\theta + \frac{1}{2})}, w2 = \frac{\Gamma(\theta + \frac{1}{2})}{2 \Gamma(\theta) + \Gamma(\theta + \frac{1}{2})}\]

# 3 Procedure, sample from $g$

Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.
```{r code, results='hide'}
gprocedure <- function(size, theta) {
  
  weight1 = 2 * gamma(theta) / (2 * gamma(theta) + gamma(theta + (1/2)))
  u = runif(size,0,1)
  sample = double(size)
  
  for (i in 1:size) {
    if (u[i] > weight1) {
      sample[i] <- rgamma(1, shape = theta, rate = 1)
    } 
    else {
      sample[i] <- rgamma(1, shape = theta + (1/2), rate = 1)
    }
  }
  return(sample)
}
gfunc <- function(theta, x) {
  g= 1/(2*gamma(theta) + gamma(theta + (1/2)))*(2*x^(theta-1) +  x^(theta-1/2))*exp(-x)
}
```
next let's assume $\theta = 3$, and $size = 10000$
```{r code2, results='hide'}
theta <- 3
size <-10000
sample <- gprocedure(size, theta)
plot(density(sample), col = "red")
curve(gfunc(theta = 3,x), add = TRUE, col = "blue")
legend("topright", legend = c("Procedure sample", "True Density"), 
      col = c("red","blue"), lty = c(1,1))
```

# 4 Rejection sampling, sample from $f$

In this section, we will use g to estimate the distribution of f , and $f(x)\varpropto q(x)=\sqrt{4+x}x^{\theta-1}e^{-x}$, and 
\begin{align*}
\alpha & =\sup_{x>0}\frac{q(x)}{g(x)}\\
       & =\sup_{x>0}\frac{\sqrt{4+x}x^{\theta-1}e^{-x}}{C(2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x}}\\
       & =\sup_{x>0}\frac{\sqrt{4+x}}{C\sqrt{x}}\\
       & =\frac{1}{C}
\end{align*}
which means
$q(x)=\sqrt{4+x}x^{\theta-1}e^{-x}\leqslant \alpha g(x)=\frac{1}{C}g(x)=(2x^{\theta-1}+x^{\theta-\frac{1}{2}})e^{-x}$
```{r code3, results='hide'}
fprocedure <- function(size, theta) {
  iter <- 0
  sample <- double(0)
  while(iter < size) {
    g <- gprocedure(1, theta)
    u <- runif(1,0,1)
    q <- sqrt(4 + g)*((g)^(theta-1))*exp(-1*g)
    r <- q/(2*g^(theta-1) +  g^(theta-1/2))*exp(-g)
    if (u <= r) { 
      sample <- append(sample, g)
      iter <- iter + 1
    }
  }
  return(sample)
}

theta <- 3
size <-10000
sample <- fprocedure(size, theta)
plot(density(sample), col = "red")
```

exercise 6.3.1

    In this assignment, we will use the Gibbs sampling approach to design an MCMC to estimate all the unknown parameters.Use the __arms()__ function in package HI.
---

# 1 Normal mixture revisited

Consider again the normal mixture example, except that the parameters of the normal distributions are considered unknown. Suppose that prior for $\mu_1$ and $\mu_2$ are $N(0, 10^2)$, that the prior for $1/\sigma_1^2$ and $1/\sigma_2^2$ are $\Gamma(a, b)$ with shape $a = .5$ and scale $ b = 10$. Further, all the priors are independent. Design an MCMC using the Gibbs sampling approach to estimate all 5 parameters. Use the __arms()__ function in package **HI**. Run your chain for sufficiently long and drop the burn-in period. Plot the histogram of the results for all the parameters.

# 2 Likelihood function

First, let's consider the normal mixture example, the normal mixture distribution is a mixture of two normal distribution function:$$N(\mu_{1}, \sigma^{2}_{1});N(\mu_{2}, \sigma^{2}_{2})$$ and the weight for each distribution is $\delta$ and $1-\delta$. 
```{r nmix-like}
mixture_distribution <- function(delta, x, mu1, mu2, sigma1, sigma2) {
    delta * dnorm(x, mu1, sigma1) + (1 - delta) * dnorm(x, mu2, sigma2)
}
```
However, we don't know these 5 parameters of the mixture distribution function. Further, all the priors are independent. Now, we will generate some random sample:$\delta=0.7, \mu_{1}=7, \sigma^{2}_{1}=0.5^{2}, \mu_{2}=10, \sigma^{2}_{2}=0.5^{2}$
```{r nmix-sim}
delta <- 0.7 # true value to be estimated based on the data
n <- 100
set.seed(123)
u <- rbinom(n, prob = delta, size = 1)
x0 <- rnorm(n, ifelse(u == 1, 7, 10), 0.5)
```
By sampling $n = 100$ data from the mixture distribution, we will get the likelihood function:
$$likelihood(x; \delta, \mu_{1}, \sigma^{2}_{1}, \mu_{2}, \sigma^{2}_{2})=\prod^{n}_{i=1}\left[\delta N(\mu_{1},\sigma^{2}_{1})+(1-\delta)N(\mu_{2}, \sigma^{2}_{2})\right]$$
so does the loglikelihood function:
$$loglike(x; \delta, \mu_{1}, \sigma^{2}_{1}, \mu_{2}, \sigma^{2}_{2}) = \sum^{n}_{i=1}log(\delta N(\mu_{1},\sigma^{2}_{1})+(1-\delta)N(\mu_{2}, \sigma^{2}_{2}))$$
```{r loglike}
loglike <- function(delta, x , mu1, mu2, sigma1, sigma2){
  sum(log(delta * dnorm(x, mu1, sigma1) + (1 - delta) * dnorm(x, mu2, sigma2)))
}
```

# 3 Posterior density

then, let's calculate posterior density, according to Bayes theory
$$p(\theta | x) = \frac{p(x|\theta)p(\theta)}{p(x)}$$
$p(\theta | x)$ is what we called posterior density, $p(x|\theta)$ is the likelihood function, $p(\theta)$ is prior probability, $p(x)$ is the normalization constant useful for Bayesian model selection. The prior for $\mu_{1}$ and $\mu_{2}$ are $N(0, 10^{2})$, the prior for $\frac{1}{\sigma^{2}_{1}}$ and $\frac{1}{\sigma^{2}_{2}}$ are $\Gamma(0.5, 10)$, hence the prior for $\sigma^{2}_{1}$ and $\sigma^{2}_{2}$ are the inverse gamma distribution $IG(0.5,10)$. \newline \newline
So we get the posterior density:
$$p(\delta, \mu_{1}, \sigma^{2}_{1}, \mu_{2}, \sigma^{2}_{2}|x) = \prod^{n}_{i=1}\left[\delta N(\mu_{1},\sigma^{2}_{1})+(1-\delta)N(\mu_{2}, \sigma^{2}_{2})\right]N(\mu_{1},0,10)N(\mu_{2},0,10)IG(\sigma^{2}_{1},0.5,10)IG(\sigma^{2}_{2},0.5,10)$$
so does the log posterior function:
$$logposterior(\delta, \mu_{1}, \sigma^{2}_{1}, \mu_{2}, \sigma^{2}_{2}|x) = \sum^{n}_{i=1}(\delta N(\mu_{1},\sigma^{2}_{1})+(1-\delta)N(\mu_{2}, \sigma^{2}_{2}))+log(N(\mu_{1},0,10))+log(N(\mu_{2},0,10))+log(IG(\sigma^{2}_{1},0.5,10))+log(IG(\sigma^{2}_{2},0.5,10))$$
```{r log posterior density}
library(invgamma)
logposterior <- function(delta, x, mu1, mu2, sigma1, sigma2){
  mu1.logprior <- dnorm(mu1, 0, 10, log = T)
  mu2.logprior <- dnorm(mu2, 0, 10, log = T)
  sigma1.logprior <- dinvgamma(sigma1^2, shape = 0.5, scale = 10, log = T)
  sigma2.logprior <- dinvgamma(sigma2^2, shape = 0.5, scale = 10, log = T)
  sum(mu1.logprior + mu2.logprior +sigma1.logprior + sigma2.logprior) + loglike(delta, x, mu1, mu2, sigma1, sigma2)
}
```

# 4 Gibbs sampling

Next we will use an MCMC based the Gibbs sampler uses the ARMS algorithm from R package HI as recommended in the question:
```{r MCMC function}
library(HI)
mymcmc <- function(niter, delta.init, mu1.init, mu2.init, sigma1.init, sigma2.init,x){
  data <- matrix(nrow = niter, ncol = 5)
  for(i in 1:niter){
    f1 <- function(x1)  logposterior(x1,x0,mu1.init,mu2.init,sigma1.init,sigma2.init)
    delta.init <- data[i,1] <- arms(delta.init, f1, function(x1) (x1 > 0) * (x1< 1), 1)
    
    f2 <- function(x2)  logposterior(delta.init,x0,x2,mu2.init,sigma1.init,sigma2.init)
    mu1.init <- data[i,2] <- arms(mu1.init, f2, function(x2) (x2 > -100) * (x2 < 100), 1)
    
    f3 <- function(x3)  logposterior(delta.init,x0,mu1.init,x3,sigma1.init,sigma2.init)
    mu2.init <- data[i,3] <- arms(mu2.init, f3, function(x3) (x3 > -100) * (x3 < 100), 1)
    
    f4 <- function(x4)  logposterior(delta.init,x0,mu1.init,mu2.init,x4,sigma2.init)
    sigma1.init <- data[i,4] <- arms(sigma1.init, f4, function(x4) (x4 > 0) * (x4 < 200), 1)
    
    f5 <- function(x5)  logposterior(delta.init,x0,mu1.init,mu2.init,sigma1.init,x5)
    sigma2.init <- data[i,5] <- arms(sigma2.init, f5, function(x5) (x5 > 0) * (x5 < 200), 1)
  }
  data
}
niter <- 3000
results <- mymcmc(niter, 0.5, 1,1,1,1,x0)
results <- results[-c(1:1000),]
options(warn=-1)
library(ggplot2)
plot1<- ggplot(data.frame(x = results[,1]), aes(x = x))+ 
  geom_histogram(aes(y=..density..))+labs(x = expression("Values of"~delta), 
  y = expression(" Density of"~delta), title=expression("Histogram of"~delta))
plot1
```
$\delta$ is about 0.7
```{r plot2}
plot2<- ggplot(data.frame(x = results[,2]), aes(x = x))+ 
  geom_histogram(aes(y=..density..))+labs(x = expression("Values of"~mu[1]), 
  y = expression(" Density of"~mu[1]), title=expression("Histogram of"~mu[1]))
plot2
```
$\mu_{1}$ is about 7
\newpage
```{r plot3}
plot3<- ggplot(data.frame(x = results[,3]), aes(x = x))+ 
  geom_histogram(aes(y=..density..))+labs(x = expression("Values of"~mu[2]), 
  y = expression(" Density of"~mu[2]), title=expression("Histogram of"~mu[2]))
plot3
```
$\mu_{2}$ is about 10
\newpage
```{r plot4}
plot4<- ggplot(data.frame(x = results[,4]), aes(x = x))+ 
  geom_histogram(aes(y=..density..))+labs(x = expression("Values of"~sigma[1]), 
  y = expression(" Density of"~sigma[1]), title=expression("Histogram of"~sigma[1]))
plot4
```
\newpage
```{r plot5}
plot5<- ggplot(data.frame(x = results[,5]), aes(x = x))+ 
  geom_histogram(aes(y=..density..))+labs(x = expression("Values of"~sigma[2]), 
  y = expression(" Density of"~sigma[2]), title=expression("Histogram of"~sigma[2]))
plot5
```










































